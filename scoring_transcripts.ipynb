{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTZrbV6UjTE1ESpmkHCGwR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somewhereovertherainbo/audio_processing/blob/main/scoring_transcripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm\n",
        "\n",
        "1. get audio file as a path\n",
        "2. Extract audio file from audio path\n",
        "3. send this file to mod_wsola algorithm to get y, inp_pos, out_pos\n",
        "4. send y to whisper model to transcribe\n",
        "5. Get the transcriptions\n",
        "6. append words, wst, wet from transcriptions\n",
        "7. process wst, wet\n",
        "8. do the reverse mapping\n",
        "9. return three important lists"
      ],
      "metadata": {
        "id": "J1EFHxs5o_dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs and imports\n",
        "\n",
        "!pip install torch librosa pydub\n",
        "import torch\n",
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "import whisper\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zvbWPEEiJqh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef12b7d-732d-4fe7-a6a4-88c4b28ef546"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.6.2)\n",
            "Installing collected packages: pydub, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pydub-0.25.1\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-d_9wurtu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-d_9wurtu\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802823 sha256=7492fd3becdcc878108ef2a74a5cace75fc6aed162793a3686cb420f8497f134\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-70t_w4s5/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile mod_wsola.py\n",
        "\n",
        "\n",
        "### Modified WSOLA Algorithm that returns input positions and output positions as well.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def win_func(win_type='hann', win_size=4096, zero_pad=0):\n",
        "    \"\"\"Generate diverse type of window function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    win_type : str\n",
        "               the type of window function.\n",
        "               Currently, Hann and Sin are supported.\n",
        "    win_size : int > 0 [scalar]\n",
        "               the size of window function.\n",
        "               It doesn't contains the length of zero padding.\n",
        "    zero_pad : int > 0 [scalar]\n",
        "               the total length of zero-pad.\n",
        "               Zeros are equally distributed\n",
        "               for both left and right of the window.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    win : numpy.ndarray([shape=(win_size)])\n",
        "          the window function generated.\n",
        "    \"\"\"\n",
        "\n",
        "    if win_type == 'hann':\n",
        "        win = np.hanning(win_size)\n",
        "    elif win_type == 'sin':\n",
        "        win = np.sin(np.pi * np.arange(win_size) / (win_size - 1))\n",
        "    else:\n",
        "        raise Exception(\"Please use the valid window type. (hann, sin)\")\n",
        "\n",
        "    win = np.pad(win, zero_pad // 2, 'constant')\n",
        "\n",
        "    return win\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "\n",
        "\n",
        "def _validate_audio(audio):\n",
        "    \"\"\"validate the input audio and modify the order of channels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    audio : numpy.ndarray [shape=(channel, num_samples) or (num_samples)\\\n",
        "                           or (num_samples, channel)]\n",
        "            the input audio sequence to validate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    audio : numpy.ndarray [shape=(channel, num_samples)]\n",
        "            the validataed output audio sequence.\n",
        "    \"\"\"\n",
        "    if audio.ndim == 1:\n",
        "        audio = np.expand_dims(audio, 0)\n",
        "    elif audio.ndim > 2:\n",
        "        raise Exception(\"Please use the valid audio source. \"\n",
        "                        + \"Number of dimension of input should be less than 3.\")\n",
        "    elif audio.shape[0] > audio.shape[1]:\n",
        "        warn('it seems that the 2nd axis of the input audio source '\n",
        "             + 'is a channel. it is recommended that fix channel '\n",
        "             + 'to the 1st axis.', stacklevel=3)\n",
        "        audio = audio.T\n",
        "\n",
        "    return audio\n",
        "\n",
        "\n",
        "def _validate_scale_factor(audio, s):\n",
        "    \"\"\"Validate the scale factor s and\n",
        "    convert the fixed scale factor to anchor points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    audio : numpy.ndarray [shape=(num_channels, num_samples) \\\n",
        "                           or (num_samples) or (num_samples, num_channels)]\n",
        "            the input audio sequence.\n",
        "    s : number > 0 [scalar] or numpy.ndarray [shape=(2, num_points) \\\n",
        "        or (num_points, 2)]\n",
        "        the time stretching factor. Either a constant value (alpha)\n",
        "        or an (2 x n) (or (n x 2)) array of anchor points\n",
        "        which contains the sample points of the input signal in the first row\n",
        "        and the sample points of the output signal in the second row.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    anc_points : numpy.ndarray [shape=(2, num_points)]\n",
        "                 anchor points which contains the sample points\n",
        "                 of the input signal in the first row\n",
        "                 and the sample points of the output signal in the second row.\n",
        "    \"\"\"\n",
        "    if np.isscalar(s):\n",
        "        anc_points = np.array([[0, np.shape(audio)[1] - 1],\n",
        "                               [0, np.ceil(s * np.shape(audio)[1]) - 1]])\n",
        "    elif s.ndim == 2:\n",
        "        if s.shape[0] == 2:\n",
        "            anc_points = s\n",
        "        elif s.shape[1] == 2:\n",
        "            warn('it seems that the anchor points '\n",
        "                 + 'has shape (num_points, 2). '\n",
        "                 + 'it is recommended to '\n",
        "                 + 'have shape (2, num_points).', stacklevel=3)\n",
        "            anc_points = s.T\n",
        "    else:\n",
        "        raise Exception('Please use the valid anchor points. '\n",
        "                        + '(scalar or pair of input/output sample points)')\n",
        "\n",
        "    return anc_points\n",
        "\n",
        "\n",
        "def _validate_f0(audio, f0):\n",
        "    \"\"\"Validate the input f0 is suitable for input audio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    audio : numpy.ndarray [shape=(num_channels, num_samples) or \\\n",
        "                           (num_samples) or (num_samples, num_channels)]\n",
        "            the input audio sequence.\n",
        "    f0 : numpy.ndarray [shape=(num_channels, num_pitches) or \\\n",
        "                        (num_pitches) or (num_pitches, num_channels)]\n",
        "         the f0 sequence that used for TD-PSOLA. If f0 is 1D array,\n",
        "         the f0 of all audio channels are regarded as the same f0.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    f0 : numpy.ndarray [shape=(num_channels, num_freqs)]\n",
        "         the f0 sequence that used for TD-PSOLA.\n",
        "    \"\"\"\n",
        "    n_chan = audio.shape[0]\n",
        "\n",
        "    if f0.ndim == 1:\n",
        "        f0 = np.tile(f0, (n_chan, 1))\n",
        "    elif f0.ndim == 2:\n",
        "        if f0.shape[0] == n_chan:\n",
        "            pass\n",
        "        elif f0.shape[1] == n_chan:\n",
        "            warn('it seems that the f0 has shape (num_pitches, num_channels). '\n",
        "                 + 'it is recommended to '\n",
        "                 + 'have shape (num_channels, num_pitches).', stacklevel=3)\n",
        "            f0 = f0.T\n",
        "        else:\n",
        "            raise Exception(\"The number of channels of f0 value \"\n",
        "                            + \"should 1 or same as the input audio.\")\n",
        "    else:\n",
        "        raise Exception(\"Please use the valid f0 value. \"\n",
        "                        + \"Number of dimension of f0 \"\n",
        "                        + \"should be less than 3.\")\n",
        "\n",
        "    return f0\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "# from .utils import win_func\n",
        "# from .utils import _validate_audio, _validate_scale_factor\n",
        "\n",
        "\n",
        "def mod_wsola(x, s, win_type='hann',\n",
        "          win_size=1024, syn_hop_size=512, tolerance=512):\n",
        "    \"\"\"Modify the length of the audio sequence using the WSOLA algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : numpy.ndarray [shape=(channel, num_samples) or (num_samples)]\n",
        "        The input audio sequence to modify.\n",
        "    s : number > 0 [scalar] or numpy.ndarray [shape=(2, num_points)]\n",
        "        The time stretching factor. Either a constant value (alpha)\n",
        "        or a 2 x n array of anchor points containing the sample points\n",
        "        of the input signal in the first row and the sample points of the\n",
        "        output signal in the second row.\n",
        "    win_type : str, optional\n",
        "               Type of the window function. Options are 'hann' and 'sin'.\n",
        "               Default is 'hann'.\n",
        "    win_size : int > 0, optional\n",
        "               Size of the window function. Default is 1024.\n",
        "    syn_hop_size : int > 0, optional\n",
        "                   Hop size of the synthesis window, usually half of the window size.\n",
        "                   Default is 512.\n",
        "    tolerance : int >= 0, optional\n",
        "                Number of samples the window positions in the input signal may\n",
        "                be shifted to avoid phase discontinuities when overlap-adding\n",
        "                them to form the output signal. Default is 512.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : numpy.ndarray [shape=(channel, num_samples) or (num_samples)]\n",
        "        The modified output audio sequence.\n",
        "    input_positions : numpy.ndarray\n",
        "                      The input positions corresponding to each output position.\n",
        "    output_positions : numpy.ndarray\n",
        "                      The output positions in the output signal.\n",
        "    \"\"\"\n",
        "    # Validate the input audio and scale factor.\n",
        "    x = _validate_audio(x)\n",
        "    anc_points = _validate_scale_factor(x, s)\n",
        "\n",
        "    n_chan = x.shape[0]\n",
        "    output_length = int(anc_points[1, -1]) + 1\n",
        "\n",
        "    win = win_func(win_type=win_type, win_size=win_size, zero_pad=0)\n",
        "\n",
        "    sw_pos = np.arange(0, output_length + win_size // 2, syn_hop_size)\n",
        "    ana_interpolated = interp1d(anc_points[1, :], anc_points[0, :],\n",
        "                                fill_value='extrapolate')\n",
        "    aw_pos = np.round(ana_interpolated(sw_pos)).astype(int)\n",
        "    ana_hop = np.insert(aw_pos[1:] - aw_pos[:-1], 0, 0)\n",
        "\n",
        "    y = np.zeros((n_chan, output_length))\n",
        "\n",
        "    min_fac = np.min(syn_hop_size / ana_hop[1:])\n",
        "\n",
        "    # Padding the input audio sequence.\n",
        "    left_pad = int(win_size // 2 + tolerance)\n",
        "    right_pad = int(np.ceil(1 / min_fac) * win_size + tolerance)\n",
        "    x_padded = np.pad(x, ((0, 0), (left_pad, right_pad)), 'constant')\n",
        "\n",
        "    aw_pos += tolerance\n",
        "\n",
        "    deltas = np.zeros(len(aw_pos))\n",
        "    input_positions = []\n",
        "    output_positions = []\n",
        "\n",
        "    # Applying WSOLA to each channel\n",
        "    for c, x_chan in enumerate(x_padded):\n",
        "        y_chan = np.zeros(output_length + 2 * win_size)\n",
        "        ow = np.zeros(output_length + 2 * win_size)\n",
        "\n",
        "        delta = 0\n",
        "\n",
        "        for i in range(len(aw_pos) - 1):\n",
        "            x_adj = x_chan[aw_pos[i] + delta: aw_pos[i] + win_size + delta]\n",
        "            y_chan[sw_pos[i]: sw_pos[i] + win_size] += x_adj * win\n",
        "            ow[sw_pos[i]: sw_pos[i] + win_size] += win\n",
        "\n",
        "            nat_prog = x_chan[aw_pos[i] + delta + syn_hop_size:\n",
        "                              aw_pos[i] + delta + syn_hop_size + win_size]\n",
        "\n",
        "            next_aw_range = np.arange(aw_pos[i+1] - tolerance,\n",
        "                                      aw_pos[i+1] + win_size + tolerance)\n",
        "\n",
        "            x_next = x_chan[next_aw_range]\n",
        "\n",
        "            cross_corr = np.correlate(nat_prog, x_next)\n",
        "            max_index = np.argmax(cross_corr)\n",
        "\n",
        "            delta = tolerance - max_index\n",
        "            deltas[i] = delta\n",
        "\n",
        "            # Record input-output position mapping\n",
        "            input_positions.append(aw_pos[i] + delta)\n",
        "            output_positions.append(sw_pos[i])\n",
        "\n",
        "        # Calculate last frame\n",
        "        x_adj = x_chan[aw_pos[-1] + delta: aw_pos[-1] + win_size + delta]\n",
        "        y_chan[sw_pos[-1]: sw_pos[-1] + win_size] += x_adj * win\n",
        "        ow[sw_pos[-1]: sw_pos[-1] + win_size] += win\n",
        "\n",
        "        ow[ow < 1e-3] = 1\n",
        "\n",
        "        y_chan = y_chan / ow\n",
        "        y_chan = y_chan[win_size // 2:]\n",
        "        y_chan = y_chan[: output_length]\n",
        "\n",
        "        y[c, :] = y_chan\n",
        "\n",
        "    # Add the last positions\n",
        "    input_positions.append(aw_pos[-1] + delta)\n",
        "    output_positions.append(sw_pos[-1])\n",
        "\n",
        "    return y.squeeze(), np.array(input_positions), np.array(output_positions)\n",
        "\n",
        "\n",
        "### Code to generate transcripts using whisper large v2 model\n",
        "\n",
        "# %%writefile whatever.py\n",
        "\n",
        "import whisper\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def generate_transcriptions(audio_path):\n",
        "\n",
        "    # def split_audio(audio, chunk_length=30, overlap=0):\n",
        "    #     audio = audio\n",
        "    #     chunks = []\n",
        "    #     step = chunk_length - overlap\n",
        "    #     for start in range(0, len(audio), step * 1000):\n",
        "    #         end = start + chunk_length * 1000\n",
        "    #         chunks.append(audio[start:end])\n",
        "    #     return chunks\n",
        "\n",
        "    def transcribe_chunks(audio_path, model, device):\n",
        "        transcriptions = []\n",
        "        # for chunk in chunks:\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        audio = torch.tensor(audio).to(device)\n",
        "        result = model.transcribe(audio, language='sa', word_timestamps=True)\n",
        "        transcriptions.append(result)\n",
        "        return transcriptions\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = whisper.load_model(\"large-v2\").to(device)\n",
        "\n",
        "    # chunks = split_audio(audio)\n",
        "    # print(f\"Split audio into {len(chunks)} chunks\")\n",
        "\n",
        "    transcriptions = transcribe_chunks(audio_path, model, device)\n",
        "    # print(f\"Transcribed chunks: {transcriptions}\")\n",
        "\n",
        "    return transcriptions\n"
      ],
      "metadata": {
        "id": "otPAKlcVCaLK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t8mHx2ldJQ04"
      },
      "outputs": [],
      "source": [
        "def get_lists(speed_factor, audio_path ):\n",
        "\n",
        "  \"\"\"\n",
        "  Slows down the audio file from audio path and splits it into 30 sec chunks.\n",
        "  These chunks are transcribed spearately using whisper large-v2 model.\n",
        "  The lists of words, word start times, word end times from each of the chunks is merged.\n",
        "  Returns lists named words, word_start_times, word_end_times.\n",
        "\n",
        "  Inputs: speed_factor, audio_path\n",
        "  Output: words, word_start_times, word_end_times\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the audio\n",
        "  audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "  # converting audio into numpy nd array\n",
        "  audio_array, sr = librosa.load(audio_path, sr= 16000)\n",
        "\n",
        "  # Slow it down for processing\n",
        "  audio, input_positions, output_positions = mod_wsola(audio_array, speed_factor)\n",
        "\n",
        "  # Generate transcripts\n",
        "  transcriptions = generate_transcriptions(audio_path)\n",
        "\n",
        "  # Get the words, word start times, word end times\n",
        "  words = []\n",
        "  word_start_times = []\n",
        "  word_end_times = []\n",
        "  for transcription in transcriptions:\n",
        "    for segment in transcription['segments']:\n",
        "      for word in segment['words']:\n",
        "        # print(word['word'])\n",
        "        words.append(word['word'])\n",
        "        # print(word['start'])\n",
        "        word_start_times.append(word['start'])\n",
        "        # print(word['end'])\n",
        "        word_end_times.append(word['end'])\n",
        "\n",
        "  # Process words, word_start_times, word_end_times\n",
        "  # idxs = np.where(np.array(word_start_times == 0))[0]\n",
        "\n",
        "  # gg = []\n",
        "  # gg.extend(word_start_times[0:idxs[0]])\n",
        "  # # print(len(gg))\n",
        "  # gg.extend(word_start_times[idxs[0]:idxs[1]]+word_end_times[idxs[0]])\n",
        "  # # print(len(gg))\n",
        "  # gg.extend(word_start_times[idxs[1]:]+word_end_times[idxs[0]]+word_end_times[idxs[1]])\n",
        "  # # print(len(gg))\n",
        "\n",
        "  # ge = []\n",
        "  # ge.extend(word_end_times[0:idxs[0]])\n",
        "  # # print(len(gg))\n",
        "  # ge.extend(word_end_times[idxs[0]:idxs[1]]+word_end_times[idxs[0]])\n",
        "  # # print(len(gg))\n",
        "  # ge.extend(word_end_times[idxs[1]:]+word_end_times[idxs[0]]+word_end_times[idxs[1]])\n",
        "\n",
        "\n",
        "  def mod_time_mapping_wsola(input_positions, output_positions):\n",
        "      \"\"\"Map time stamps from the input to the stretched output using exact positions.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      input_positions : numpy.ndarray\n",
        "                        The input positions corresponding to each output position.\n",
        "      output_positions : numpy.ndarray\n",
        "                        The output positions in the output signal.\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      input_to_output : function\n",
        "                        A function that maps input time stamps to output time stamps.\n",
        "      output_to_input : function\n",
        "                        A function that maps output time stamps to input time stamps.\n",
        "      \"\"\"\n",
        "      # Create a list of tuples for quick lookup\n",
        "      position_map = list(zip(input_positions, output_positions))\n",
        "\n",
        "      # Define the mapping functions\n",
        "      def input_to_output(input_time):\n",
        "        # Find the closest input position\n",
        "        closest_idx = np.argmin(np.abs(input_positions - input_time))\n",
        "        return output_positions[closest_idx]\n",
        "\n",
        "      def output_to_input(output_time):\n",
        "        # Find the closest output position\n",
        "        closest_idx = np.argmin(np.abs(output_positions - output_time))\n",
        "        return input_positions[closest_idx]\n",
        "\n",
        "      return input_to_output, output_to_input\n",
        "\n",
        "\n",
        "  word_start_times_reversed = []\n",
        "  word_end_times_reversed = []\n",
        "\n",
        "  input_to_output, output_to_input = mod_time_mapping_wsola(input_positions, output_positions)\n",
        "\n",
        "  for i in word_start_times:\n",
        "    word_start_times_reversed.append(output_to_input(i*sr)/sr)\n",
        "\n",
        "  for i in word_end_times:\n",
        "    word_end_times_reversed.append(output_to_input(i*sr)/sr)\n",
        "\n",
        "  return words, word_start_times_reversed, word_end_times_reversed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w, wst, wet = get_lists(float(9/10), audio_path = '/content/Kanda4_Mantra_no_sil_pydub_sil_0004.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFTi-T3kDqDp",
        "outputId": "1ef78d6b-d32f-4916-e056-7e99b59cee16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:28<00:00, 109MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a,b,c in zip(w,wst,wet):\n",
        "  print(a,b,c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni6LCfKg19BR",
        "outputId": "bdfe52a2-f7b7-4e5c-d65d-56e7056e5c5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " सही 0.0284375 0.4239375\n",
            " दिवस्सपुरूतिव्यारूतस्था 0.4239375 2.5570625\n",
            " महीक्षेमंरोदासी 2.5570625 4.302\n",
            " असकाभागतु 4.302 5.1955625\n",
            " महान 5.1955625 5.90775\n",
            " मही 5.90775 6.2596875\n",
            " असकाभागत्विजातो 6.2596875 7.9501875\n",
            " द्याम् 7.9501875 8.5739375\n",
            " सत्वपार्थीवन्चरजाहम् 8.5739375 9.4255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def match(a, b):\n",
        "  return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "sent = ''.join(w)\n",
        "# print(sent)\n",
        "\n",
        "match(sent, 'स हि दि॒वः स पृ॑थि॒व्या ऋ॑त॒स्था म॒ही क्षेमं॒ रोद॑सी अस्कभायत् म॒हान्म॒ही अस्क॑भाय॒द्वि जा॒तो द्यां सद्म॒ पार्थि॑वं च॒ रजः॑')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OU5dXG42aAF",
        "outputId": "4ea5a95d-d524-4a8e-9673-3770897204ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6782608695652174"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5t7esYN2szP",
        "outputId": "70cb6b02-54ef-4b2d-9161-9964c73c64cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' सही',\n",
              " ' दिवस्सपुरूतिव्यारूतस्था',\n",
              " ' महीक्षेमंरोदासी',\n",
              " ' असकाभागतु',\n",
              " ' महान',\n",
              " ' मही',\n",
              " ' असकाभागत्विजातो',\n",
              " ' द्याम्',\n",
              " ' सत्वपार्थीवन्चरजाहम्']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transcriptions = generate_transcriptions('/content/Kanda4_Mantra_no_sil_pydub_sil_0004.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlyiaMDNQRJG",
        "outputId": "672f7111-0d2b-4651-b24f-8b817bc55d0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:28<00:00, 109MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = librosa.load('/content/Kanda4_Mantra_no_sil_pydub_sil_0004.wav', sr = 16000)"
      ],
      "metadata": {
        "id": "BHtI9HLcb3kN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_score(lists):\n",
        "#   ###\n",
        "#   return score"
      ],
      "metadata": {
        "id": "qa6iNjQNNzhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}